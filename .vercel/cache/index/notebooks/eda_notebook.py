# -*- coding: utf-8 -*-
"""EDA Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_nKzgrSnQnVmOd3IAqqh-n03RRVOm1lr
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from collections import Counter
import re #RegEX
from sentence_transformers import SentenceTransformer, util
import torch
nltk.download('stopwords')

#  Data Loading and Inspection
data_file = '/content/DataNeuron_Text_Similarity.csv'

originaldata=pd.read_csv(data_file)



df = pd.read_csv(data_file)
print(f"Dataset '{data_file}' loaded successfully!")
print("\nFirst 5 rows of the dataset:")
print(df.head())
print("\nDataset Info:")
df.info()
print("\nMissing values per column:")
print(df.isnull().sum())

print("\n--- Initial Data Inspection Complete ---")


print("*"*100)

print("\n--- Text Length Analysis ---")

df['text1_char_len'] = df['text1'].apply(len)
df['text2_char_len'] = df['text2'].apply(len)

# Calculate word length for each paragraph
# Using a simple split by space; for more robust word counting, consider NLTK's word_tokenize
df['text1_word_len'] = df['text1'].apply(lambda x: len(x.split()))
df['text2_word_len'] = df['text2'].apply(lambda x: len(x.split()))

print("\nCharacter Lengths (text1):")
print(df['text1_char_len'].describe())
print("\nCharacter Lengths (text2):")
print(df['text2_char_len'].describe())

print("\nWord Lengths (text1):")
print(df['text1_word_len'].describe())
print("\nWord Lengths (text2):")
print(df['text2_word_len'].describe())

print("\nFirst 5 rows with length columns:")
print(df[['text1', 'text1_char_len', 'text1_word_len', 'text2', 'text2_char_len', 'text2_word_len']].head())

print("\n--- Visualizing Text Length Distributions ---")

sns.set_style("whitegrid")

fig, axes = plt.subplots(2, 2, figsize=(25, 10))
fig.suptitle('Distribution of Paragraph Lengths', fontsize=16)

sns.histplot(df['text1_char_len'], bins=50, kde=True, ax=axes[0, 0], color='skyblue')
axes[0, 0].set_title('Text1 Character Length Distribution')
axes[0, 0].set_xlabel('Character Length')
axes[0, 0].set_ylabel('Frequency')

sns.histplot(df['text2_char_len'], bins=50, kde=True, ax=axes[0, 1], color='lightcoral')
axes[0, 1].set_title('Text2 Character Length Distribution')
axes[0, 1].set_xlabel('Character Length')
axes[0, 1].set_ylabel('Frequency')

sns.histplot(df['text1_word_len'], bins=50, kde=True, ax=axes[1, 0], color='mediumseagreen')
axes[1, 0].set_title('Text1 Word Length Distribution')
axes[1, 0].set_xlabel('Word Length')
axes[1, 0].set_ylabel('Frequency')

sns.histplot(df['text2_word_len'], bins=50, kde=True, ax=axes[1, 1], color='darkorange')
axes[1, 1].set_title('Text2 Word Length Distribution')
axes[1, 1].set_xlabel('Word Length')
axes[1, 1].set_ylabel('Frequency')

plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to prevent title overlap
plt.show()

print("\n--- Text Length Visualizations Complete ---")

"""- Seems to follow a right skewed normal distribution.

- slight problem as sentence-transformers library (and the underlying Transformer models like BERT) typically have a maximum input sequence length (e.g., 512 tokens).

- The avg word cnt here is around 400 ish words. Thats okay, but.
- The max word counts (3345 for text1, 4492 for text2) are very high.
-The mean, median, and quartiles are very similar between text1 and text2 columns which says taht the data sampling process for both columns was consistent, and we don't have a bias where one side of the pair is systematically shorter or longer.
----

- Based on these, Better to use SBERT.
"""

print("\n--- Text Content Analysis ---")

# --- 1. Basic Preprocessing for Tokenization (simple) ---
def simple_tokenize(text):
    text = text.lower() #LowCAse CONVERSION
    text = re.sub(r'[^a-z\s]', '', text) # Only alphabetic characters
    words = text.split() # Split by space
    return words

# Apply simple tokenization to both columns
all_words_text1 = []
df['text1'].apply(lambda x: all_words_text1.extend(simple_tokenize(x)))

all_words_text2 = []
df['text2'].apply(lambda x: all_words_text2.extend(simple_tokenize(x)))

all_words = all_words_text1 + all_words_text2 # Combine all words for overall vocabulary

# --- 2. Vocabulary Size ---
vocabulary = set(all_words)
print(f"\nTotal unique words (vocabulary size): {len(vocabulary)}")

# --- 3. Most Common Words (Overall) ---
print("\nTop 20 Most Common Words (Overall):")
word_counts = Counter(all_words)
for word, count in word_counts.most_common(20):
    print(f"- {word}: {count}")

# Visualize common words (optional but good for intuition)
# Exclude obvious stop words if you want to see more meaningful terms
common_words_df = pd.DataFrame(word_counts.most_common(20), columns=['word', 'count'])

plt.figure(figsize=(12, 6))
sns.barplot(x='count', y='word', data=common_words_df, palette='viridis')
plt.title('Top 20 Most Common Words')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()


# --- 4. N-grams (Bigrams and Trigrams) ---
# N-grams give us common phrases, not just individual words
from nltk.util import ngrams
from nltk.corpus import stopwords
# You might need to download stopwords if you haven't already
# import nltk
# nltk.download('stopwords')

# Define English stop words
stop_words = set(stopwords.words('english'))

def generate_ngrams(text_list, n, num_top_ngrams=20):
    tokens = [word for word in text_list if word.isalpha() and word not in stop_words]
    n_grams = ngrams(tokens, n)
    counts = Counter(n_grams)
    return counts.most_common(num_top_ngrams)

print("\nTop 20 Most Common Bigrams (excluding stopwords):")
top_bigrams = generate_ngrams(all_words, 2)
for bigram, count in top_bigrams:
    print(f"- {' '.join(bigram)}: {count}")

print("\nTop 20 Most Common Trigrams (excluding stopwords):")
top_trigrams = generate_ngrams(all_words, 3)
for trigram, count in top_trigrams:
    print(f"- {' '.join(trigram)}: {count}")

print("\n--- Text Content Analysis Complete ---")

"""- Vocab size is 25737.
- Generic stop words are the,to,of,and,in,s.


---

### Insights of Dataset

- This is a news corpus, mainly from the BBC, focusing on British politics, sports (especially rugby), and general current affairs/finance
- This detailed understanding further makes  my  decision to use Sentence-BERT  even stronger.
-
"""

df = pd.read_csv(data_file)
def clean_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

print("\nApplying basic text cleaning...")
df['text1_cleaned'] = df['text1'].apply(clean_text)
df['text2_cleaned'] = df['text2'].apply(clean_text)
print("Text cleaning complete. Showing first 5 cleaned rows:")
print(df[['text1_cleaned', 'text2_cleaned']].head())

# Load the pre-trained Sentence-BERT model
print("\nLoading pre-trained Sentence-BERT model 'all-MiniLM-L6-v2'...")
try:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("Model loaded successfully!")
    # Check if GPU is available and being used (optional, but good practice)
    if torch.cuda.is_available():
        print(f"Using GPU for encoding: {torch.cuda.get_device_name(0)}")
    else:
        print("GPU not available, using CPU for encoding.")
except Exception as e:
    print(f"Error loading Sentence-BERT model: {e}")
    print("Please ensure you have an active internet connection or the model is cached.")
    exit()

"""#Cosine Similarity formula thingy"""

#  Measuring the Closeness (Cosine Similarity)

def calculate_semantic_similarity(paragraph1: str, paragraph2: str, model: SentenceTransformer) -> float:
    """
    Calculates the semantic similarity between two paragraphs using a Sentence-BERT model.

    Args:
        paragraph1 (str): The first text paragraph.
        paragraph2 (str): The second text paragraph.
        model (SentenceTransformer): The pre-trained Sentence-BERT model.

    Returns:
        float: The semantic similarity score between 0 and 1.
               (1 means highly similar, 0 means highly dissimilar)
    """
    # Encode the paragraphs into embeddings
    embeddings1 = model.encode(paragraph1, convert_to_tensor=True) #to_tensor makes it a PyTorch Tensor thingy.
    embeddings2 = model.encode(paragraph2, convert_to_tensor=True)

    # Compute cosine similarity between the embeddings
    cosine_score = util.cos_sim(embeddings1, embeddings2).item()
    #Normalizing score so that it so that, -1 to 0 => 0

    # Normalize the score from [-1, 1] to [0, 1] as per the problem requirement
    # This maps -1 to 0, 0 to 0.5, and 1 to 1.
    normalized_similarity = (cosine_score + 1) / 2

    return normalized_similarity

# Applying  the Similarity Function
print("\nCalculating semantic similarity scores for each paragraph pair...")
df['similarity_score'] = df.apply(
    lambda row: calculate_semantic_similarity(row['text1_cleaned'], row['text2_cleaned'], model),
    axis=1
)

print("Similarity calculation complete!")

# --- Initial Sanity Check and Display Results ---
print("\nDataFrame with calculated similarity scores (first 10 rows):")
# Display text1, text2, and the new similarity_score
print(df[['text1', 'text2', 'similarity_score']].head(10))

# Also display some examples with very high and very low scores to intuitively check
print("\nExamples with highest similarity scores:")
print(df.nlargest(5, 'similarity_score')[['text1', 'text2', 'similarity_score']])

print("\nExamples with lowest similarity scores:")
print(df.nsmallest(5, 'similarity_score')[['text1', 'text2', 'similarity_score']])

# --- Save Results (Optional but Recommended) ---
output_file = 'paragraphs_with_semantic_similarity.csv'
df.to_csv(output_file, index=False)
print(f"\nResults saved to '{output_file}'")

print("\n--- Semantic Similarity Model Development (Part A) Complete! ---")